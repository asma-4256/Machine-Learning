ğŸ“Œ What are Optimization Algorithms?

ğŸ”–They improve how Gradient Descent updates model parameters.
ğŸ”–Help achieve faster convergence and avoid local minima.


ğŸ“ŒCommon Optimization Algorithms:

ğŸ”¹ Gradient Descent â€“ The fundamental optimization approach.
ğŸ”¹ Momentum â€“ Uses past gradients to accelerate learning.
ğŸ”¹ AdaGrad â€“ Adapts learning rate for each parameter.
ğŸ”¹ RMSprop â€“ Addresses AdaGradâ€™s limitations by maintaining a moving average of squared gradients.
ğŸ”¹ Adam â€“ Combines Momentum and RMSprop for efficient optimization.

Loss Function in machine learning:

ğŸ”¹What is a Loss Function?

    ğŸ“ŒIt measures the difference between the predicted values and the actual values in a machine learning model.
    ğŸ“ŒThe goal is to minimize this loss to improve model accuracy.

ğŸ”¹Example:

    Suppose we have input ğ‘¥ and output ğ‘¦. 
    A simple loss function could be:
            ğ¿ ( ğ‘¦ , ğ‘¦^) = ( ğ‘¦ âˆ’ ğ‘¦^) ^2
 
        Here, 
        y ( ğ‘¦ ) - is the actual value, 

        y^ ( ğ‘¦^) - is the predicted value, and ğ¿ is the loss.


Gradient Descent in machine learning

ğŸ“ŒWhat is Gradient Descent?

    Itâ€™s an optimization algorithm used to minimize the loss function by iteratively updating the modelâ€™s parameters (weights and biases).

ğŸ“ŒHow It Works?

    Compute the gradient of the loss function.
    Adjust the parameters in the opposite direction of the gradient.
    Repeat the process until the loss is minimized.

**Hyperparameter in Gradient Descent optimization â€“ ğ‹ğğšğ«ğ§ğ¢ğ§ğ  ğ‘ğšğ­ğ!


ğŸ“ŒWhat is Learning Rate (Î±)?

    ğŸ“It controls how much the modelâ€™s parameters are updated in each step of Gradient Descent.
    ğŸ“A small learning rate results in slow convergence.
    ğŸ“A large learning rate may overshoot the minimum or cause instability.


ğŸ“ŒChoosing the Right Learning Rate:

    ğŸ“Too small â†’ Takes too long to converge.
    ğŸ“Too large â†’ May never converge and oscillate.
    ğŸ“Optimal learning rate â†’ Balances speed and stability.

**Types of Gradient Descent in machine learning!

    ğŸ“ŒBatch Gradient Descent:

        Uses the entire dataset to compute gradients for each update.
        More stable but computationally expensive for large datasets.


    ğŸ“ŒStochastic Gradient Descent (SGD):

        Updates parameters using one data point at a time.
        Faster but introduces noise, leading to more fluctuations.


    ğŸ“ŒMini-Batch Gradient Descent:

        Uses small batches of data for updates.
        Balances efficiency and stability, making it widely used in deep learning.