ğğ¯ğğ«ğŸğ¢ğ­ğ­ğ¢ğ§ğ 
ğŸ“Œ What is Overfitting?
  When a model performs exceptionally well on training data but poorly on test data.

ğŸ“Œ Causes:
  Model complexity, insufficient training data, noise or irrelevant features.

ğŸ“Œ Prevention:
  Use more data, simplify the model, apply regularization, use dropout, and perform cross-validation.

ğ”ğ§ğğğ«ğŸğ¢ğ­ğ­ğ¢ğ§ğ 
ğŸ“Œ What is Underfitting?
  When a model performs poorly on both training and test data because it is too simple to capture the patterns in the data.

ğŸ“Œ Causes:
  Model is too simple, insufficient training time, or incorrect features used.

ğŸ“Œ Prevention:
  Increase model complexity, train for a longer time, and select relevant features.

  

Bias-Variance Tradeoff in machine learning:

ğŸ”¹ What are Bias and Variance?

        Bias: The error due to overly simplistic models, leading to underfitting.
        Variance: The error due to overly complex models, leading to overfitting.


ğŸ”¹ How do Bias and Variance relate to ML?

        High bias = underfitting (model misses key patterns).
        High variance = overfitting (model captures noise instead of patterns).


ğŸ”¹ Techniques to Improve the Bias-Variance Tradeoff:

    ğŸ“ŒUse cross-validation to optimize model complexity.
    ğŸ“ŒSelect the right algorithm for the problem.
    ğŸ“ŒRegularization techniques like L1 or L2 to balance model complexity.
    ğŸ“ŒCollect more data to reduce variance.


