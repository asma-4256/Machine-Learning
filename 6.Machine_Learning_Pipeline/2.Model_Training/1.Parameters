 Parameters in Machine Learning:

ğŸ”¹ Types of Parameters:

Model Parameters:

    These are learned from the data during training, such as:
    ğ–ğğ¢ğ ğ¡ğ­ğ¬: Determine the importance of features.
    ğğ¢ğšğ¬: Adjusts the output along with the weights to better fit the data.

 ğŸ”¹ Weights:

    ğŸ“ŒDefine how much influence an input has on the output.
    ğŸ“ŒAdjusted during training to minimize the loss function and improve predictions.


 ğŸ”¹ Bias:

    ğŸ“ŒAn offset value added to the model.
    ğŸ“ŒHelps the model make accurate predictions by shifting the activation function.

Hyperparameters:

    These are set before training and control the learning process, such as:
    ğ‹ğğšğ«ğ§ğ¢ğ§ğ  ğ‘ğšğ­ğ: Determines how quickly a model learns.
    ğğ®ğ¦ğ›ğğ« ğ¨ğŸ ğ„ğ©ğ¨ğœğ¡ğ¬: Specifies how many times the model will see the entire dataset during training.

 ğŸ“Œ Learning Rate:

    Controls how quickly the model learns.

 ğŸ“Œ Number of Epochs:

    Determines how many times the model will see the entire dataset during training.

Methods of Hyper Tuning:

ğŸ“ŒGridSearchCV ğŸ”

    Tries all possible hyperparameter combinations.

    Best for smaller datasets with fewer hyperparameters.

    Downside? Computationally expensive for large datasets.

ğŸ“ŒRandomizedSearchCV ğŸ¯

    Randomly selects a subset of hyperparameter combinations.

    Best for large datasets where exhaustive search is impractical.

    Faster & efficient, with a good chance of finding optimal values.

âœ¨Which one to choose?

        âœ…If speed matters â†’ Use RandomizedSearchCV

        âœ…If accuracy is top priority â†’ Use GridSearchCV (for smaller models)


        

ğŸ”„ Model Training Steps :

        1.Preprocess Data â€“ Clean, encode, scale, and split.

        2.Feed Data â€“ Pass inputs and labels into the model.

        3.Forward Pass â€“ Model makes predictions.

        4.Loss Calculation â€“ Compare predictions with actual values.

        5.Backpropagation â€“ Compute gradients (errors).

        6.Weight Update â€“ Adjust model weights using optimization (e.g., gradient descent).

        7.Repeat â€“ Do this for many epochs until loss decreases.

        8.Validate â€“ Check performance on validation data to avoid overfitting.
